<!DOCTYPE HTML>
<HTML>
  <HEAD>
    <TITLE>zdl5</TITLE>
    <META CHARSET='UTF-8'>
    <META NAME='auhtor' CONTENT='hyotang666'>
    <META NAME='generator' CONTENT='pages'>
    <LINK REL='stylesheet' HREF='../css/css.css' TYPE='text/css'>
  </HEAD>
  <BODY>
    <MAIN>
      <h1>ゼロから作るDeep Learning.Common Lispで学ぶディープラーニングの理論と実装(5)</h1>

<p>原著の内容をCommon Lispに移植しながら学んでいくシリーズです。
詳細は原著でお読みください。
ここでは移植したCommon Lispコードについての解説や注意点を記していきます。</p>

<h1>5 Back propagation.</h1>

<h2>5.1 Computational graph.</h2>

<h3>5.1.1 With computational graph.</h3>

<h3>5.1.2 Local computation.</h3>

<h3>5.1.3 Why.</h3>

<h2>5.2 Chain rule.</h2>

<h3>5.2.1 Back propagation for computational graph.</h3>

<h3>5.2.2 What is.</h3>

<h3>5.2.3 Chain rules and computational graph.</h3>

<h2>5.3 Back propagation.</h2>

<h3>5.3.1 Back propagation for add layer.</h3>

<h3>5.3.2 Back propagation for multiply layer.</h3>

<h3>5.3.3 Example.</h3>

<h3>5.4 Implementation of simple layer.</h3>

<h4>5.4.1 Implementation of multiply layer.</h4>

<p><code>FORWARD</code>と<code>BACKWARD</code>は本来メソッドとして実装したいところです。
しかしながらCommon Lispのメソッドはシグネチャが共通の構造を持たねばならないという制約があります。
すなわち、あるクラスへのメソッドは引数が２つだが別なクラスへのメソッドは引数が１つである、というような定義をするのは難しいのです。
不可能ではないのですが、その場合は引数全てを&amp;RESTで受けて本体内で分配するという手段をとらねばなりません。</p>

<p>ここではForward、Backward両関数をスロットに格納することにします。
こうすることで各関数は自由なラムダリストを持てます。</p>

<pre><code><span class="code"><span class="comment">;; 抽象クラス。
</span><span class="comment">;; 各ConcreteクラスがForward、Backwardスロットを定義しなければエラー。
</span><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i> layer
  <span class="paren2">(<span class="code">forward <span class="paren3">(<span class="code">error <span class="string">"Forward slot is required."</span></span>)</span> <span class="keyword">:type</span> <i><span class="symbol">function</span></i></span>)</span>
  <span class="paren2">(<span class="code">backward <span class="paren3">(<span class="code">error <span class="string">"Backward slot is required."</span></span>)</span> <span class="keyword">:type</span> <i><span class="symbol">function</span></i></span>)</span></span>)</span>

<span class="comment">;; 表示が長くなりすぎないように。
</span><span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> print-object <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">o layer</span>)</span> stream</span>)</span>
  <span class="paren2">(<span class="code">print-unreadable-object <span class="paren3">(<span class="code">o stream <span class="keyword">:type</span> t</span>)</span></span>)</span></span>)</span>

<span class="comment">;; インターフェースとしてのFORWARD関数。
</span><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> forward <span class="paren2">(<span class="code">layer &amp;rest args</span>)</span>
  <span class="paren2">(<span class="code">apply <span class="paren3">(<span class="code">layer-forward layer</span>)</span> layer args</span>)</span></span>)</span>

<span class="comment">;; インターフェースとしてのBACKWARD関数。
</span><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> backward <span class="paren2">(<span class="code">layer &amp;rest args</span>)</span>
  <span class="paren2">(<span class="code">apply <span class="paren3">(<span class="code">layer-backward layer</span>)</span> layer args</span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i> 
    <span class="paren2">(<span class="code">*-layer
     <span class="comment">;; LAYER 構造体を継承。
</span>     <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
     <span class="comment">;; 独自のコンストラクタを定義。
</span>     <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-*-layer
      <span class="paren4">(<span class="code">&amp;key x y
       <span class="comment">;; &amp;AUX 経由で継承先のスロット値を指定。
</span>       <span class="comment">;; &amp;KEYと異なりAPIとしては公開されないのでエンドユーザーは初期値を指定できない。
</span>       &amp;aux
       <span class="paren5">(<span class="code">forward <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x y</span>)</span>
                  <span class="paren1">(<span class="code">setf <span class="paren2">(<span class="code">*-layer-x this</span>)</span> x
                        <span class="paren2">(<span class="code">*-layer-y this</span>)</span> y</span>)</span>
                  <span class="paren1">(<span class="code">numcl:* x y</span>)</span></span>)</span></span>)</span>
       <span class="paren5">(<span class="code">backward <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span>
                   <span class="paren1">(<span class="code">values <span class="paren2">(<span class="code">numcl:* x <span class="paren3">(<span class="code">*-layer-y this</span>)</span></span>)</span>
                           <span class="paren2">(<span class="code">numcl:* x <span class="paren3">(<span class="code">*-layer-x this</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  x
  y</span>)</span>

* <span class="paren1">(<span class="code"><i><span class="symbol">let*</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">apple 100</span>)</span>
         <span class="paren3">(<span class="code">apple-num 2</span>)</span>
         <span class="paren3">(<span class="code">tax 1.1</span>)</span>
         <span class="paren3">(<span class="code">*-apple-layer <span class="paren4">(<span class="code">make-*-layer</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">*-tax-layer <span class="paren4">(<span class="code">make-*-layer</span>)</span></span>)</span>
         <span class="comment">;; Forward.
</span>         <span class="paren3">(<span class="code">apple-price <span class="paren4">(<span class="code">forward *-apple-layer apple apple-num</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">price <span class="paren4">(<span class="code">print <span class="paren5">(<span class="code">forward *-tax-layer apple-price tax</span>)</span></span>)</span></span>)</span>
         <span class="comment">;; Backward.
</span>         <span class="paren3">(<span class="code">dprice 1</span>)</span></span>)</span>
    <span class="paren2">(<span class="code">multiple-value-bind <span class="paren3">(<span class="code">dapple-price dtax</span>)</span> <span class="paren3">(<span class="code">backward *-tax-layer dprice</span>)</span>
      <span class="paren3">(<span class="code">multiple-value-bind <span class="paren4">(<span class="code">dapple dapple-num</span>)</span> <span class="paren4">(<span class="code">backward *-apple-layer dapple-price</span>)</span>
        <span class="paren4">(<span class="code">format t <span class="string">"~S ~S ~S"</span> dapple dapple-num dtax</span>)</span></span>)</span></span>)</span></span>)</span>
220.0 2.2 110.0 200
NIL</span></code></pre>

<h3>5.4.2 Implementation of add layer.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i>
    <span class="paren2">(<span class="code">+-layer <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
     <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-+-layer
      <span class="paren4">(<span class="code">&amp;aux
       <span class="paren5">(<span class="code">forward
         <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x y</span>)</span> <span class="paren1">(<span class="code">declare <span class="paren2">(<span class="code">ignore this</span>)</span></span>)</span> <span class="paren1">(<span class="code">numcl:+ x y</span>)</span></span>)</span></span>)</span>
       <span class="paren5">(<span class="code">backward
         <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span> <span class="paren1">(<span class="code">declare <span class="paren2">(<span class="code">ignore this</span>)</span></span>)</span> <span class="paren1">(<span class="code">values x x</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>

* <span class="paren1">(<span class="code"><i><span class="symbol">let*</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">apple 100</span>)</span>
         <span class="paren3">(<span class="code">apple-num 2</span>)</span>
         <span class="paren3">(<span class="code">orange 150</span>)</span>
         <span class="paren3">(<span class="code">orange-num 3</span>)</span>
         <span class="paren3">(<span class="code">tax 1.1</span>)</span>
         <span class="comment">;; Layers
</span>         <span class="paren3">(<span class="code">*-apple-layer <span class="paren4">(<span class="code">make-*-layer</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">*-orange-layer <span class="paren4">(<span class="code">make-*-layer</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">apple+orange <span class="paren4">(<span class="code">make-+-layer</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">*-tax-layer <span class="paren4">(<span class="code">make-*-layer</span>)</span></span>)</span>
         <span class="comment">;; Forward.
</span>         <span class="paren3">(<span class="code">apple-price <span class="paren4">(<span class="code">forward *-apple-layer apple apple-num</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">orange-price <span class="paren4">(<span class="code">forward *-orange-layer orange orange-num</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">all-price <span class="paren4">(<span class="code">forward apple+orange apple-price orange-price</span>)</span></span>)</span>
         <span class="paren3">(<span class="code">price <span class="paren4">(<span class="code">forward *-tax-layer all-price tax</span>)</span></span>)</span>
         <span class="comment">;; Backward.
</span>         <span class="paren3">(<span class="code">dprice 1</span>)</span></span>)</span>
    <span class="paren2">(<span class="code">multiple-value-bind <span class="paren3">(<span class="code">dall-price dtax</span>)</span> <span class="paren3">(<span class="code">backward *-tax-layer dprice</span>)</span>
      <span class="paren3">(<span class="code">multiple-value-bind <span class="paren4">(<span class="code">dapple-price dorange-price</span>)</span> <span class="paren4">(<span class="code">backward apple+orange dall-price</span>)</span>
        <span class="paren4">(<span class="code">multiple-value-bind <span class="paren5">(<span class="code">dorange dorange-num</span>)</span> <span class="paren5">(<span class="code">backward *-orange-layer dorange-price</span>)</span>
          <span class="paren5">(<span class="code">multiple-value-bind <span class="paren6">(<span class="code">dapple dapple-num</span>)</span> <span class="paren6">(<span class="code">backward *-apple-layer dapple-price</span>)</span>
            <span class="paren6">(<span class="code">format t <span class="string">"~%~S~%~S ~S ~S ~S ~S"</span>
                    price
                    dapple-num dapple
                    dorange dorange-num
                    dtax</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
715.0
110.0 2.2 3.3000002 165.0 650</span></code></pre>

<h2>5.5 Implementation of activation layer.</h2>

<h3>5.5.1 ReLU layer.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i>
    <span class="paren2">(<span class="code">relu-layer <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
     <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-relu-layer
      <span class="paren4">(<span class="code">&amp;key mask
       &amp;aux
        <span class="paren5">(<span class="code">forward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span>
            <span class="paren1">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">mask <span class="paren4">(<span class="code">numcl:&lt;= x 0</span>)</span></span>)</span></span>)</span>
              <span class="paren2">(<span class="code">setf <span class="paren3">(<span class="code">relu-layer-mask this</span>)</span> mask</span>)</span>
              <span class="paren2">(<span class="code">numcl:* mask x</span>)</span></span>)</span></span>)</span></span>)</span>
        <span class="paren5">(<span class="code">backward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span> <span class="paren1">(<span class="code">numcl:* <span class="paren2">(<span class="code">relu-layer-mask this</span>)</span> x</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  mask</span>)</span>

<span class="comment">;; Print/Read 同一性を保護。
</span><span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> print-object <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">o relu-layer</span>)</span> stream</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">if</span></i> <span class="paren3">(<span class="code">not <span class="special">*print-readably*</span></span>)</span>
    <span class="paren3">(<span class="code">call-next-method</span>)</span>
    <span class="paren3">(<span class="code">format stream <span class="string">"#.~S"</span>
            `<span class="paren4">(<span class="code">make-relu-layer <span class="keyword">:mask</span> ,<span class="paren5">(<span class="code">relu-layer-mask o</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<h3>5.5.2 Sigmoid layer.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i>
    <span class="paren2">(<span class="code">sigmoid-layer <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
     <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-sigmoid-layer
      <span class="paren4">(<span class="code">&amp;key out
       &amp;aux
        <span class="paren5">(<span class="code">forward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span>
            <span class="paren1">(<span class="code">setf <span class="paren2">(<span class="code">sigmoid-layer-out this</span>)</span>
                  <span class="paren2">(<span class="code">numcl:/ 1 <span class="paren3">(<span class="code">numcl:+ 1 <span class="paren4">(<span class="code">numcl:exp <span class="paren5">(<span class="code">numcl:- x</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
        <span class="paren5">(<span class="code">backward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x</span>)</span>
            <span class="paren1">(<span class="code">numcl:* x
                     <span class="paren2">(<span class="code">numcl:- 1.0 <span class="paren3">(<span class="code">sigmoid-layer-out this</span>)</span></span>)</span>
                     <span class="paren2">(<span class="code">sigmoid-layer-out this</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  out</span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> print-object <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">o sigmoid-layer</span>)</span> stream</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">if</span></i> <span class="paren3">(<span class="code">not <span class="special">*print-readably*</span></span>)</span>
    <span class="paren3">(<span class="code">call-next-method</span>)</span>
    <span class="paren3">(<span class="code">format stream <span class="string">"#.~S"</span>
            `<span class="paren4">(<span class="code">make-sigmoid-layer <span class="keyword">:out</span> ,<span class="paren5">(<span class="code">sigmoid-layer-out o</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<h2>5.6 Implementation of Affine/Softmax layer.</h2>

<h3>5.6.1 Affine layer.</h3>

<h3>5.6.2 Batch version.</h3>

<pre><code><span class="code">* <span class="paren1">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">x.w <span class="paren4">(<span class="code">numcl:asarray '<span class="paren5">(<span class="code"><span class="paren6">(<span class="code">0 0 0</span>)</span> <span class="paren6">(<span class="code">10 10 10</span>)</span></span>)</span></span>)</span></span>)</span>
        <span class="paren3">(<span class="code">b <span class="paren4">(<span class="code">numcl:asarray '<span class="paren5">(<span class="code">1 2 3</span>)</span></span>)</span></span>)</span></span>)</span>
    <span class="paren2">(<span class="code">numcl:+ x.w b</span>)</span></span>)</span>
#2A<span class="paren1">(<span class="code"><span class="paren2">(<span class="code">1 2 3</span>)</span> <span class="paren2">(<span class="code">11 12 13</span>)</span></span>)</span>

* <span class="paren1">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">dy <span class="paren4">(<span class="code">numcl:asarray '<span class="paren5">(<span class="code"><span class="paren6">(<span class="code">1 2 3</span>)</span> <span class="paren6">(<span class="code">4 5 6</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
    <span class="paren2">(<span class="code">numcl:sum dy <span class="keyword">:axes</span> 0</span>)</span></span>)</span>
#<span class="paren1">(<span class="code">5 7 9</span>)</span></span></code></pre>

<p>原著ではネットワークオブジェクトがParameterをハッシュで持っていましたが、ここでは<code>LAYER</code>そのものに持たせることとします。
変数のスコープを小さく保つのが目的です。
（巨大なグローバル変数を持ちたくない）</p>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i>
  <span class="paren2">(<span class="code">affine-layer <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
   <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-affine-layer
    <span class="paren4">(<span class="code">&amp;key weight bias input dw db
     &amp;aux
     <span class="paren5">(<span class="code">forward
       <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this input</span>)</span>
         <span class="paren1">(<span class="code">setf <span class="paren2">(<span class="code">affine-layer-input this</span>)</span> input</span>)</span>
         <span class="paren1">(<span class="code">numcl:+ <span class="paren2">(<span class="code">dot input <span class="paren3">(<span class="code">affine-layer-weight this</span>)</span></span>)</span>
                  <span class="paren2">(<span class="code">affine-layer-bias this</span>)</span></span>)</span></span>)</span></span>)</span>
     <span class="paren5">(<span class="code">backward
       <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this dout</span>)</span>
         <span class="paren1">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">dx <span class="paren4">(<span class="code">dot dout <span class="paren5">(<span class="code">numcl:transpose <span class="paren6">(<span class="code">affine-layer-weight this</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
           <span class="paren2">(<span class="code">setf <span class="paren3">(<span class="code">affine-layer-dw this</span>)</span>
                   <span class="paren3">(<span class="code">dot <span class="paren4">(<span class="code">numcl:transpose <span class="paren5">(<span class="code">affine-layer-input this</span>)</span></span>)</span>
                        dout</span>)</span>
                 <span class="paren3">(<span class="code">affine-layer-db this</span>)</span>
                   <span class="paren3">(<span class="code">numcl:sum dout <span class="keyword">:axes</span> 0</span>)</span></span>)</span>
           dx</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  weight
  bias
  input
  dw
  db</span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> print-object <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">o affine-layer</span>)</span> stream</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">if</span></i> <span class="paren3">(<span class="code">not <span class="special">*print-readably*</span></span>)</span>
    <span class="paren3">(<span class="code">call-next-method</span>)</span>
    <span class="paren3">(<span class="code">format stream <span class="string">"#.~S"</span>
            `<span class="paren4">(<span class="code">make-affine-layer <span class="keyword">:weight</span> ,<span class="paren5">(<span class="code">affine-layer-weight o</span>)</span>
                                <span class="keyword">:bias</span> ,<span class="paren5">(<span class="code">affine-layer-bias o</span>)</span>
                                <span class="keyword">:input</span> ,<span class="paren5">(<span class="code">affine-layer-input o</span>)</span>
                                <span class="keyword">:dw</span> ,<span class="paren5">(<span class="code">affine-layer-dw o</span>)</span>
                                <span class="keyword">:db</span> ,<span class="paren5">(<span class="code">affine-layer-db o</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<h3>5.6.3 Softmax with loss layer.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defstruct</span></i>
    <span class="paren2">(<span class="code">softmax-with-loss-layer <span class="paren3">(<span class="code"><span class="keyword">:include</span> layer</span>)</span>
     <span class="paren3">(<span class="code"><span class="keyword">:constructor</span> make-softmax-with-loss-layer
      <span class="paren4">(<span class="code">&amp;key loss y teach
       &amp;aux
        <span class="paren5">(<span class="code">forward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this x teach</span>)</span>
            <span class="paren1">(<span class="code">setf <span class="paren2">(<span class="code">softmax-with-loss-layer-teach this</span>)</span> teach
                  <span class="paren2">(<span class="code">softmax-with-loss-layer-y this</span>)</span> <span class="paren2">(<span class="code">softmax x</span>)</span>
                  <span class="paren2">(<span class="code">softmax-with-loss-layer-loss this</span>)</span>
                    <span class="paren2">(<span class="code">cross-entropy-error <span class="paren3">(<span class="code">softmax-with-loss-layer-y this</span>)</span>
                                         teach</span>)</span></span>)</span></span>)</span></span>)</span>
        <span class="paren5">(<span class="code">backward
          <span class="paren6">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren1">(<span class="code">this &amp;optional <span class="paren2">(<span class="code">x 1</span>)</span></span>)</span>
            <span class="paren1">(<span class="code">numcl:/ <span class="paren2">(<span class="code">numcl:- <span class="paren3">(<span class="code">softmax-with-loss-layer-y this</span>)</span>
                              <span class="paren3">(<span class="code">softmax-with-loss-layer-teach this</span>)</span></span>)</span>
                     <span class="paren2">(<span class="code">nth 0 <span class="paren3">(<span class="code">numcl:shape <span class="paren4">(<span class="code">softmax-with-loss-layer-teach this</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  loss
  y
  teach</span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> print-object <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">o softmax-with-loss-layer</span>)</span> stream</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">if</span></i> <span class="paren3">(<span class="code">not <span class="special">*print-readably*</span></span>)</span>
    <span class="paren3">(<span class="code">call-next-method</span>)</span>
    <span class="paren3">(<span class="code">format stream <span class="string">"#.~S"</span>
            `<span class="paren4">(<span class="code">make-softmax-with-loss-layer <span class="keyword">:loss</span> ,<span class="paren5">(<span class="code">softmax-with-loss-loss o</span>)</span>
                                           <span class="keyword">:y</span> ,<span class="paren5">(<span class="code">softmax-with-loss-y o</span>)</span>
                                           <span class="keyword">:teach</span> ,<span class="paren5">(<span class="code">softmax-with-loss-teach o</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<h2>5.7 Implementation of back propagation.</h2>

<h3>5.7.1 Overall of learnings of neural network.</h3>

<h3>5.7.2 Implementation of neural network which supports back propagation.</h3>

<p>PREDICTは出力LAYERへの入力値を返す関数です。</p>

<p>Haskellなら<code>swap</code>を使いたいところ。</p>

<p><code>BUTLAST</code>で毎回中間リストがアロケートされるのも非効率ですが、ここでは保守性を取ることとします。</p>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> predict <span class="paren2">(<span class="code">network input</span>)</span>
  <span class="paren2">(<span class="code">reduce <span class="paren3">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren4">(<span class="code">input layer</span>)</span> <span class="paren4">(<span class="code">forward layer input</span>)</span></span>)</span>
          <span class="paren3">(<span class="code">butlast network</span>)</span>
          <span class="keyword">:initial-value</span> input</span>)</span></span>)</span></span></code></pre>

<p><code>PROPAGATE</code>は<code>PREDICT</code>の反対のようなものです。
本関数の目的は引数<code>NETWORK</code>が保持する各<code>LAYER</code>オブジェクトのスロット値を破壊変更することです。
なので返り値は<code>NETWORK</code>とします。</p>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> propagate <span class="paren2">(<span class="code">network dout</span>)</span>
  <span class="paren2">(<span class="code">reduce #'backward network <span class="keyword">:initial-value</span> dout <span class="keyword">:from-end</span> t</span>)</span>
  network</span>)</span></span></code></pre>

<p><code>LOSS</code>も同様に<code>LAYER</code>の破壊変更が目的なので<code>NETWORK</code>を返します。</p>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> loss <span class="paren2">(<span class="code">network input label</span>)</span>
  <span class="paren2">(<span class="code">forward <span class="paren3">(<span class="code">car <span class="paren4">(<span class="code">last network</span>)</span></span>)</span> <span class="paren3">(<span class="code">predict network input</span>)</span> label</span>)</span>
  network</span>)</span></span></code></pre>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> accuracy <span class="paren2">(<span class="code">network input label</span>)</span>
  <span class="paren2">(<span class="code">when <span class="paren3">(<span class="code">/= 1 <span class="paren4">(<span class="code">array-rank input</span>)</span></span>)</span>
    <span class="paren3">(<span class="code">setf label <span class="paren4">(<span class="code">numcl:asarray <span class="paren5">(<span class="code">argmax label <span class="keyword">:axes</span> 1</span>)</span></span>)</span></span>)</span></span>)</span>
  <span class="paren2">(<span class="code">numcl:/ <span class="paren3">(<span class="code">numcl:sum <span class="paren4">(<span class="code">numcl:= <span class="paren5">(<span class="code">numcl:asarray <span class="paren6">(<span class="code">argmax <span class="paren1">(<span class="code">predict network input</span>)</span>
                                                      <span class="keyword">:axes</span> 1</span>)</span></span>)</span>
                               label</span>)</span></span>)</span>
           <span class="paren3">(<span class="code">float <span class="paren4">(<span class="code">nth 0 <span class="paren5">(<span class="code">numcl:shape input</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> compute-numerical-gradient <span class="paren2">(<span class="code">network input label</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">flet</span></i> <span class="paren3">(<span class="code"><span class="paren4">(<span class="code">loss-w <span class="paren5">(<span class="code">weight</span>)</span>
           <span class="paren5">(<span class="code">declare <span class="paren6">(<span class="code">ignore weight</span>)</span></span>)</span>
           <span class="paren5">(<span class="code">loss network input label</span>)</span></span>)</span></span>)</span>
    <span class="paren3">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:for</span> layer <span class="keyword">:in</span> network
          <span class="keyword">:collect</span> <span class="paren4">(<span class="code">numerical-gradient #'loss-w
                                       <span class="paren5">(<span class="code">affine-layer-weight layer</span>)</span></span>)</span>
          <span class="keyword">:collect</span> <span class="paren4">(<span class="code">numerical-gradient #'loss-w
                                       <span class="paren5">(<span class="code">affine-layer-bias layer</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> gradient <span class="paren2">(<span class="code">network input label</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:for</span> layer <span class="keyword">:in</span> <span class="paren3">(<span class="code">propagate <span class="paren4">(<span class="code">loss network input label</span>)</span> 1</span>)</span>
        <span class="keyword">:when</span> <span class="paren3">(<span class="code">affine-layer-p layer</span>)</span>
        <span class="keyword">:collect</span> <span class="paren3">(<span class="code">affine-layer-dw layer</span>)</span>
        <span class="keyword">:and</span> <span class="keyword">:collect</span> <span class="paren3">(<span class="code">affine-layer-db layer</span>)</span></span>)</span></span>)</span></span></code></pre>

<h3>5.7.3 Gradient checks of back propagation.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> make-net <span class="paren2">(<span class="code">specs</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">flet</span></i> <span class="paren3">(<span class="code"><span class="paren4">(<span class="code">constructor <span class="paren5">(<span class="code">name</span>)</span>
           <span class="paren5">(<span class="code">uiop:find-symbol* <span class="paren6">(<span class="code">format nil <span class="string">"MAKE-~A-LAYER"</span> name</span>)</span> <span class="special">*package*</span></span>)</span></span>)</span></span>)</span>
    <span class="paren3">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:for</span> <span class="paren4">(<span class="code">main shape activator</span>)</span> <span class="keyword">:in</span> specs
          <span class="keyword">:for</span> size <span class="keyword">:=</span> nil
          <span class="comment">;; Trivial syntax-check.
</span>          <span class="keyword">:do</span> <span class="paren4">(<span class="code">check-type main symbol</span>)</span>
              <span class="paren4">(<span class="code">check-type shape <span class="paren5">(<span class="code">cons integer <span class="paren6">(<span class="code">cons integer null</span>)</span></span>)</span></span>)</span>
              <span class="paren4">(<span class="code">check-type activator symbol</span>)</span>
              <span class="paren4">(<span class="code">when size
                <span class="paren5">(<span class="code">assert <span class="paren6">(<span class="code">= size <span class="paren1">(<span class="code">car shape</span>)</span></span>)</span></span>)</span></span>)</span>
              <span class="paren4">(<span class="code">setf size <span class="paren5">(<span class="code">cadr shape</span>)</span></span>)</span>
          <span class="comment">;; The body.
</span>          <span class="keyword">:collect</span> <span class="paren4">(<span class="code">funcall <span class="paren5">(<span class="code">constructor main</span>)</span>
                            <span class="keyword">:weight</span> <span class="paren5">(<span class="code">numcl:normal 0.0d0 1.0d0 shape
                                                  'single-float</span>)</span>
                            <span class="keyword">:bias</span> <span class="paren5">(<span class="code">numcl:zeros <span class="paren6">(<span class="code">cadr shape</span>)</span></span>)</span></span>)</span>
          <span class="keyword">:collect</span> <span class="paren4">(<span class="code">funcall <span class="paren5">(<span class="code">constructor activator</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<pre><code><span class="code">* <span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*mnist*</span> <span class="paren2">(<span class="code">cl-mnist:load-mnist <span class="keyword">:flatten</span> t
                                             <span class="keyword">:normalize</span> t
                                             <span class="keyword">:one-hot-label</span> t
                                             <span class="keyword">:slurp</span> t</span>)</span></span>)</span>
*MNIST*

* <span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*net*</span> <span class="paren2">(<span class="code">make-net '<span class="paren3">(<span class="code"><span class="paren4">(<span class="code">affine <span class="paren5">(<span class="code">784 50</span>)</span> relu</span>)</span>
                                  <span class="paren4">(<span class="code">affine <span class="paren5">(<span class="code">50 10</span>)</span> softmax-with-loss</span>)</span></span>)</span></span>)</span></span>)</span>
*NET*

* <span class="paren1">(<span class="code">gradient <span class="special">*net*</span>
            <span class="paren2">(<span class="code">numcl:asarray <span class="paren3">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:repeat</span> 2 <span class="keyword">:collect</span> <span class="paren4">(<span class="code">funcall <span class="paren5">(<span class="code">getf <span class="special">*mnist*</span> <span class="keyword">:train-images</span></span>)</span></span>)</span></span>)</span></span>)</span>
            <span class="paren2">(<span class="code">numcl:asarray <span class="paren3">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:repeat</span> 2 <span class="keyword">:collect</span> <span class="paren4">(<span class="code">funcall <span class="paren5">(<span class="code">getf <span class="special">*mnist*</span> <span class="keyword">:train-labels</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
</span></code></pre>

<h3>5.7.4 Learnings with back propagation.</h3>

<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*learning-rate*</span> 0.01</span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> learn <span class="paren2">(<span class="code">network input label</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">loop</span></i> <span class="keyword">:for</span> layer <span class="keyword">:in</span> <span class="paren3">(<span class="code">propagate <span class="paren4">(<span class="code">loss network input label</span>)</span> 1</span>)</span>
        <span class="keyword">:when</span> <span class="paren3">(<span class="code">affine-layer-p layer</span>)</span>
        <span class="keyword">:do</span> <span class="paren3">(<span class="code">setf <span class="paren4">(<span class="code">affine-layer-weight layer</span>)</span>
                    <span class="paren4">(<span class="code">numcl:- <span class="paren5">(<span class="code">affine-layer-weight layer</span>)</span>
                             <span class="paren5">(<span class="code">numcl:* <span class="special">*learning-rate*</span> <span class="paren6">(<span class="code">affine-layer-dw layer</span>)</span></span>)</span></span>)</span>
                  <span class="paren4">(<span class="code">affine-layer-bias layer</span>)</span>
                    <span class="paren4">(<span class="code">numcl:- <span class="paren5">(<span class="code">affine-layer-bias layer</span>)</span>
                             <span class="paren5">(<span class="code">numcl:* <span class="special">*learning-rate*</span> <span class="paren6">(<span class="code">affine-layer-db layer</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
  network</span>)</span></span></code></pre>

<h2>5.8 Summary</h2>

    </MAIN>
    <FOOTER><A HREF='../indexes/index.html'>Index</A></FOOTER>
  </BODY>
</HTML>